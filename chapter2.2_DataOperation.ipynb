{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tensor\n",
    "在深度学习中，我们通常会频繁地对数据进行操作。作为动手学深度学习的基础，本节将介绍如何对内存中的数据进行操作。\n",
    "\n",
    "在PyTorch中，torch.Tensor是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现Tensor和NumPy的多维数组非常类似。然而，Tensor提供GPU计算和自动求梯度等更多功能，这些使Tensor更加适合深度学习。\n",
    "\n",
    "> \"tensor\"这个单词一般可译作“张量”，张量可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1、 创建Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 直接创建\n",
    "1. torch.Tensor(sizes)\n",
    "2. torch.tensor(data, dtype=None, device=None,requires_grad=False)\n",
    "\n",
    "  - data - 可以是list, tuple, numpy array, scalar或其他类型\n",
    "  - dtype - 可以返回想要的tensor类型\n",
    "  - device - 可以指定返回的设备\n",
    "  - requires_grad - 可以指定是否进行记录图的操作，默认为False\n",
    "  \n",
    "**注意：torch.tensor 总是会复制 data, 如果想避免复制，可以使 data.detach()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "创建一个5x3的未初始化的Tensor："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.2755e-39, 1.0561e-38, 4.5001e-39],\n",
       "        [4.2246e-39, 1.0286e-38, 1.0653e-38],\n",
       "        [1.0194e-38, 8.4490e-39, 1.0469e-38],\n",
       "        [9.3674e-39, 9.9184e-39, 8.7245e-39],\n",
       "        [9.2755e-39, 8.9082e-39, 9.9184e-39]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "创建一个5x3的未初始化的Tensor："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.9803e-16, 7.8473e-43, 5.9803e-16],\n",
      "        [7.8473e-43, 5.9803e-16, 7.8473e-43],\n",
      "        [5.9803e-16, 7.8473e-43, 5.9803e-16],\n",
      "        [7.8473e-43, 5.9803e-16, 7.8473e-43],\n",
      "        [5.9803e-16, 7.8473e-43, 5.9803e-16]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个5x3的long型全0的Tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以直接根据数据创建:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 3.]) tensor([5., 3.]) tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor((5,3))\n",
    "b = torch.Tensor([5,3])  # 两个结果相等\n",
    "print(a,b,a==b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以通过现有的Tensor来创建，此方法会默认重用输入Tensor的一些属性，例如数据类型，除非自定义数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.2424, -0.5536,  0.3723],\n",
      "        [ 0.3653,  0.5084,  0.4734],\n",
      "        [-0.0159, -1.2814,  1.2615],\n",
      "        [ 0.3237,  0.2417, -1.2923],\n",
      "        [-0.3165, -2.0940,  2.4526]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过shape或者size()来获取Tensor的形状:\n",
    "> 注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有很多函数可以创建Tensor，去翻翻官方API就知道了，下表给了一些常用的作参考。\n",
    "\n",
    "|函数\t|功能|\n",
    "| :- - -  | :- - - |\n",
    "|Tensor(sizes)\t| 基础构造函数|\n",
    "|tensor(data,)\t|类似np.array的构造函数|\n",
    "|ones(sizes)\t|全1Tensor|\n",
    "|zeros(sizes)\t|全0Tensor|\n",
    "|eye(sizes)\t|对角线为1，其他为0|\n",
    "|arange(s,e,step)\t|从s到e，步长为step|\n",
    "|linspace(s,e,steps)\t|从s到e，均匀切分成steps份|\n",
    "|rand/randn(\\*sizes)\t|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)\t|正态分布/均匀分布|\n",
    "|randperm(m)\t|随机排列|\n",
    "\n",
    "*注：这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 从numpy中获得数据\n",
    "\n",
    "**注意：这种方式会共享内存，修改numpy数据会导致tensor数据改变,而任何对tensor的操作都会影响到ndarry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4] \t tensor([1, 2, 3, 4], dtype=torch.int32)\n",
      "[1 2 3 5] \t tensor([1, 2, 3, 5], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4])\n",
    "b = torch.from_numpy(a)\n",
    "print(a,'\\t',b)\n",
    "\n",
    "a[3] = 5\n",
    "print(a,'\\t',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 创建特定的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **根据数值要求：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(*sizes, out=None, …)# 返回大小为sizes的零矩阵\n",
    "\n",
    "torch.zeros_like(input, …) # 返回与input相同size的零矩阵\n",
    "\n",
    "torch.ones(*sizes, out=None, …) #f返回大小为sizes的单位矩阵\n",
    "\n",
    "torch.ones_like(input, …) #返回与input相同size的单位矩阵\n",
    "\n",
    "torch.full(size, fill_value, …) #返回大小为sizes,单位值为fill_value的矩阵\n",
    "\n",
    "torch.full_like(input, fill_value, …) 返回与input相同size，单位值为fill_value的矩阵\n",
    "\n",
    "torch.arange(start=0, end, step=1, …) #返回从start到end, 单位步长为step的1-d tensor.\n",
    "\n",
    "torch.linspace(start, end, steps=100, …) #返回从start到end, 间隔中的插值数目为steps的1-d tensor\n",
    "\n",
    "torch.logspace(start, end, steps=100, …) #返回1-d tensor ，从10start到10end的steps个对数间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **根据矩阵要求:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.eye(n, m=None, out=None,…) #返回2-D 的单位对角矩阵\n",
    "\n",
    "torch.empty(*sizes, out=None, …) #返回被未初始化的数值填充，大小为sizes的tensor\n",
    "\n",
    "torch.empty_like(input, …) # 返回与input相同size,并被未初始化的数值填充的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **随机采用生成:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.normal(mean, std, out=None)\n",
    "\n",
    "torch.rand(*size, out=None, dtype=None, …) #返回[0,1]之间均匀分布的随机数值\n",
    "\n",
    "torch.rand_like(input, dtype=None, …) #返回与input相同size的tensor, 填充均匀分布的随机数值\n",
    "\n",
    "torch.randint(low=0, high, size,…) #返回均匀分布的[low,high]之间的整数随机值\n",
    "\n",
    "torch.randint_like(input, low=0, high, dtype=None, …) #\n",
    "\n",
    "torch.randn(*sizes, out=None, …) #返回大小为size,由均值为0，方差为1的正态分布的随机数值\n",
    "\n",
    "torch.randn_like(input, dtype=None, …)\n",
    "\n",
    "torch.randperm(n, out=None, dtype=torch.int64) # 返回0到n-1的数列的随机排列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 操作\n",
    "本小节介绍Tensor的各种操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1算术操作\n",
    "\n",
    "* **基本运算，加减乘除**  \n",
    "在PyTorch中，同一种操作可能有很多种形式，下面用加法作为例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 形式一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0197, 0.7444, 1.4426],\n",
      "        [1.3822, 1.3228, 0.8300],\n",
      "        [0.4532, 1.4033, 1.0209],\n",
      "        [1.2170, 0.2429, 0.4638],\n",
      "        [1.4893, 1.6903, 1.6833]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 形式二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0197, 0.7444, 1.4426],\n",
      "        [1.3822, 1.3228, 0.8300],\n",
      "        [0.4532, 1.4033, 1.0209],\n",
      "        [1.2170, 0.2429, 0.4638],\n",
      "        [1.4893, 1.6903, 1.6833]])\n",
      "tensor([[1.0197, 0.7444, 1.4426],\n",
      "        [1.3822, 1.3228, 0.8300],\n",
      "        [0.4532, 1.4033, 1.0209],\n",
      "        [1.2170, 0.2429, 0.4638],\n",
      "        [1.4893, 1.6903, 1.6833]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x,y))\n",
    "\n",
    "# 或指定输出\n",
    "\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 形式三 inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0197, 0.7444, 1.4426],\n",
      "        [1.3822, 1.3228, 0.8300],\n",
      "        [0.4532, 1.4033, 1.0209],\n",
      "        [1.2170, 0.2429, 0.4638],\n",
      "        [1.4893, 1.6903, 1.6833]])\n"
     ]
    }
   ],
   "source": [
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注：PyTorch操作inplace版本都有后缀_, 例如x.copy_(y), x.t_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     运算       |    Opera      |\n",
    "| :---------------:  | :---------------: |\n",
    "|  减 |y-x <br> torch.sub(y, x) <br> y.sub(x) <br>  y.sub_(x) |\n",
    "|  元素乘 |x * y <br> torch.mul(x, y) <br> x.mul(y), <br> x.mul_(y) |\n",
    "|  元素除 |x / y <br> torch.div(x, y) <br> x.div(y), <br> x.div_(y) |\n",
    "|  矩阵乘 |torch.matmul(x, y.T))) <br> (m @ n)|\n",
    "|  矩阵除 |torch.linalg.inv() |\n",
    "\n",
    "**注：（对于高维的Tensor(dim>2),定义其矩阵乘法仅在最后的两个维度上,要求前面的维度必须保持一致运算操作只有torch.matmul()和@**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 高等运算\n",
    "* **对数运算：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8607, -0.1685,  0.0179],\n",
       "        [-0.0918,  0.0662, -0.8899],\n",
       "        [ 0.0332,  0.0850, -0.2049],\n",
       "        [ 0.2308, -0.8082,  0.7295],\n",
       "        [-0.2799,  0.1523, -0.1190]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(x, out=None)  # y_i=log_e(x_i)\n",
    "\n",
    "torch.log1p(x, out=None)  #y_i=log_e(x_i+1)\n",
    "\n",
    "torch.log2(x, out=None)   #y_i=log_2(x_i)\n",
    "\n",
    "torch.log10(x,out=None)  #y_i=log_10(x_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **幂函数：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(x, 2, out=None)  # y_i=x^(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **指数运算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4776e-01, 9.7092e-01, 1.8353e+00],\n",
       "        [1.2469e+00, 2.2045e+00, 1.3752e-01],\n",
       "        [1.9427e+00, 2.3747e+00, 8.6615e-01],\n",
       "        [4.4817e+00, 1.6826e-01, 2.1255e+02],\n",
       "        [6.9036e-01, 3.1373e+00, 1.1391e+00]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x, out=None)     # y_i=e^(x_i)\n",
    "\n",
    "torch.expm1(x, out=None)   # y_i=e^(x_i) -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **截断函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ceil(x, out=None)   #返回向正方向取得最小整数\n",
    "\n",
    "torch.floor(x, out=None)  #返回向负方向取得最大整数\n",
    "\n",
    "torch.round(x, out=None) #返回相邻最近的整数，四舍五入\n",
    "torch.trunc(x, out=None) #返回整数部分数值\n",
    "torch.frac(x, out=None) #返回小数部分数值\n",
    "\n",
    "torch.fmod(x, divisor, out=None) #返回input/divisor的余数\n",
    "torch.remainder(x, divisor, out=None) #同上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **对比操作：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.eq(input, other, out=None)  #按成员进行等式操作，相同返回1\n",
    "\n",
    "torch.equal(tensor1, tensor2) #如果tensor1和tensor2有相同的size和elements，则为true\n",
    "\n",
    "torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\n",
    "\n",
    "tensor([[ 1,  0],[ 0,  1]], dtype=torch.uint8)\n",
    "\n",
    "torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\n",
    "\n",
    "tensor([[ 1,  0],[ 0,  1]], dtype=torch.uint8)\n",
    "\n",
    "\n",
    "\n",
    "torch.ge(input, other, out=None) # input>= other\n",
    "torch.gt(input, other, out=None) # input>other\n",
    "torch.le(input, other, out=None) # input=<other\n",
    "torch.lt(input, other, out=None) # input<other\n",
    "torch.ne(input, other, out=None) # input != other 不等于"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max() # 返回最大值\n",
    "torch.min() # 返回最小值\n",
    "torch.isnan(tensor) #判断是否为’nan’\n",
    "torch.sort(input, dim=None, descending=False, out=None) #对目标input进行排序\n",
    "torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) #沿着指定维度返回最大k个数值及其索引值\n",
    "torch.kthvalue(input, k, dim=None, deepdim=False, out=None) #沿着指定维度返回最小k个数值及其索引值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dot(tensor1, tensor2) #返回tensor1和tensor2的点乘\n",
    "\n",
    "torch.mm(mat1, mat2, out=None) #返回矩阵mat1和mat2的乘积\n",
    "\n",
    "torch.eig(a, eigenvectors=False, out=None) #返回矩阵a的特征值/特征向量\n",
    "\n",
    "torch.det(A) #返回矩阵A的行列式\n",
    "\n",
    "torch.trace(input) #返回2-d 矩阵的迹(对对角元素求和)\n",
    "\n",
    "torch.diag(input, diagonal=0, out=None) #\n",
    "\n",
    "torch.histc(input, bins=100, min=0, max=0, out=None) #计算input的直方图\n",
    "\n",
    "torch.tril(input, diagonal=0, out=None) #返回矩阵的下三角矩阵，其他为0\n",
    "\n",
    "torch.triu(input, diagonal=0, out=None) #返回矩阵的上三角矩阵，其他为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **tensor索引**  \n",
    "  **注意：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1378, 1.6785, 2.0422])\n",
      "tensor([1.1378, 1.6785, 2.0422])\n"
     ]
    }
   ],
   "source": [
    "y = x[0, :]\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0, :]) # 源tensor也被改了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了常用的索引选择数据之外，PyTorch还提供了一些高级的选择函数:\n",
    "\n",
    "|函数 |功能|\n",
    "| :-------------: | :-------------: |\n",
    "|index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列|\n",
    "|masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor进行选取|\n",
    "|nonzero(input)|非0元素的下标|\n",
    "|gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **改变形状**  \n",
    "\n",
    "  用view()来改变Tensor的形状：\n",
    "  \n",
    "  **注意:view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view。\n",
    "\n",
    "* 使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8622, -0.3215,  0.0422],\n",
      "        [-1.1905, -0.8354, -1.8711],\n",
      "        [-0.9207, -0.7837, -1.3761],\n",
      "        [-0.2986, -1.8445,  3.3639],\n",
      "        [-1.4751, -0.5800, -1.2396]])\n",
      "tensor([ 0.1378,  0.6785,  1.0422, -0.1905,  0.1646, -0.8711,  0.0793,  0.2163,\n",
      "        -0.3761,  0.7014, -0.8445,  4.3639, -0.4751,  0.4200, -0.2396])\n"
     ]
    }
   ],
   "source": [
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **tensor 转numpy**\n",
    "\n",
    "  * item():它可以将一个标量Tensor转换成一个Python number\n",
    "  * numpy():它可以将一个多维Tensor转换成一个Python number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8622, -0.3215,  0.0422],\n",
      "        [-1.1905, -0.8354, -1.8711],\n",
      "        [-0.9207, -0.7837, -1.3761],\n",
      "        [-0.2986, -1.8445,  3.3639],\n",
      "        [-1.4751, -0.5800, -1.2396]])\n",
      "0.28911006450653076\n",
      "[[-0.86218345 -0.3215015   0.04215813]\n",
      " [-1.1904652  -0.8354379  -1.8711458 ]\n",
      " [-0.9206618  -0.7836976  -1.3761202 ]\n",
      " [-0.2985927  -1.8444823   3.3638597 ]\n",
      " [-1.4750576  -0.5799607  -1.2396071 ]]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(torch.rand(1).item())\n",
    "print(x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、 广播机制\n",
    "前面我们看到如何对两个形状相同的Tensor做按元素运算。当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于x和y分别是1行2列和3行1列的矩阵，如果要计算x + y，那么x中第一行的2个元素被广播（复制）到了第二行和第三行，而y中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、运算的内存开销\n",
    "索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。为了演示这一点，我们可以使用Python自带的id函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before) # False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想指定结果到原来的y的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把x + y的结果通过[:]写进y对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以使用运算符全名函数中的out参数或者自加运算符+=(也即add_())达到上述效果，例如torch.add(x, y, out=y)和y += x(y.add_(x))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y) # y += x, y.add_(x)\n",
    "print(id(y) == id_before) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注：虽然view返回的Tensor与源Tensor是共享data的，但是依然是一个新的Tensor（因为Tensor除了包含data外还有一些其他属性），二者id（内存地址）并不一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、 Tensor和NumPy相互转换\n",
    "我们很容易用numpy()和from_numpy()将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！\n",
    "\n",
    "> 还有一个常用的将NumPy中的array转换成Tensor的方法就是torch.tensor(), 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的Tensor和原来的数据不再共享内存。\n",
    "\n",
    "所有在CPU上的Tensor（除了CharTensor）都支持与NumPy数组相互转换。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6、Tensor on GPU\n",
    "    用方法to()可以将Tensor在CPU和GPU（需要硬件支持）之间相互移动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7、小技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.tensor设置判断:\n",
    "torch.is_tensor()  #如果是pytorch的tensor类型返回true\n",
    "\n",
    "torch.is_storage() # 如果是pytorch的storage类型返回ture\n",
    "\n",
    "# 2.判断tensor是否为空，可以如下\n",
    "a=torch.Tensor()\n",
    "len(a)\n",
    "# or\n",
    "len(a) is 0\n",
    "\n",
    "# 3.设置: 通过一些内置函数，可以实现对tensor的精度, 类型，print打印参数等进行设置\n",
    "\n",
    "torch.set_default_dtype(d)  #对torch.tensor() 设置默认的浮点类型\n",
    "\n",
    "torch.set_default_tensor_type() # 同上，对torch.tensor()设置默认的tensor类型\n",
    "torch.tensor([1.2, 3]).dtype # initial default for floating point is torch.float32\n",
    "torch.float32\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.tensor([1.2, 3]).dtype # a new floating point tensor\n",
    "torch.float64\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.tensor([1.2, 3]).dtype # a new floating point tensor\n",
    "torch.float64\n",
    "\n",
    "torch.get_default_dtype() #获得当前默认的浮点类型torch.dtype\n",
    "torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None）#)\n",
    "\n",
    "                       \n",
    "# tensor的一些用法：\n",
    "torch.lerp(star, end, weight) : 返回结果是out= star t+ (end-start) * weight\n",
    "torch.rsqrt(input) : 返回平方根的倒数\n",
    "torch.mean(input) : 返回平均值\n",
    "torch.std(input) : 返回标准偏差\n",
    "torch.prod(input) : 返回所有元素的乘积\n",
    "torch.sum(input) : 返回所有元素的之和\n",
    "torch.var(input) : 返回所有元素的方差\n",
    "torch.tanh(input) ：返回元素双正切的结果\n",
    "torch.equal(torch.Tensor(a), torch.Tensor(b)) ：两个张量进行比较，如果相等返回true\n",
    "torch.max(input)： 返回输入元素的最大值\n",
    "torch.min(input) ： 返回输入元素的最小值\n",
    "element_size() ：返回单个元素的字节\n",
    "torch.from_numpy(obj)，利用一个numpy的array创建Tensor。注意，若obj原来是1列或者1行，无论obj是否为2维，所生成的Tensor都是一阶的，若需要2阶的Tensor，需要利用view()函数进行转换。\n",
    "torch.numel(obj)，返回Tensor对象中的元素总数。\n",
    "torch.ones_like(input)，返回一个全1的Tensor，其维度与input相一致\n",
    "torch.cat(seq, dim)，在给定维度上对输入的张量序列进行连接操作\n",
    "torch.chunk（input, chunks, dim）在给定维度(轴)上将输入张量进行分块\n",
    "torch.squeeze(input)，将input中维度数值为1的维度去除。可以指定某一维度。共享input的内存\n",
    "torch.unsqeeze(input, dim)，在input目前的dim维度上增加一维\n",
    "torch.clamp(input, min, max)，将input的值约束在min和max之间\n",
    "torch.trunc(input)，将input的小数部分舍去\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附录\n",
    "\n",
    "#### 解决cuda不支持torch.inverse()的难点。\n",
    "参考：[pytorch实现矩阵的逆](https://blog.csdn.net/zhou_438/article/details/115355584?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecpm-1-115355584.pc_agg_new_rank&utm_term=pytorch%E9%80%86%E7%9F%A9%E9%98%B5&spm=1000.2123.3001.4430)\n",
    "\n",
    "背景：本来pytorch是实现了逆矩阵的函数的，1.8之前是torch.inverse()，1.8是在torch.linalg.inv()  \n",
    "但是，cuda不支持逆矩阵inverse，因此onnx当然也没实现inverse,但是我们训练的模型需要移植到onnx甚至cuda就会变得很困难，甚至你想在pytorch里面使用model.half()进行半精度运算的时候，就会报错说，cuda不支持inverse\n",
    "\n",
    "因此，只能换个思路，自己实现这个算子，使用更为常见的算子来进行替代inverse\n",
    "\n",
    "pytorch和numpy的源码很难找到inverse的具体实现，那我可以直接改为pytorch版的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0800, -0.2000,  0.4400],\n",
      "        [ 0.3200,  0.2000, -0.2400],\n",
      "        [-0.2800,  0.2000, -0.0400]])\n",
      "tensor([[ 0.0800, -0.2000,  0.4400],\n",
      "        [ 0.3200,  0.2000, -0.2400],\n",
      "        [-0.2800,  0.2000, -0.0400]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.linalg import det\n",
    " \n",
    "def cof1(M,index):\n",
    "    zs = M[:index[0]-1,:index[1]-1]\n",
    "    ys = M[:index[0]-1,index[1]:]\n",
    "    zx = M[index[0]:,:index[1]-1]\n",
    "    yx = M[index[0]:,index[1]:]\n",
    "    s = torch.cat((zs,ys),axis=1)\n",
    "    x = torch.cat((zx,yx),axis=1)\n",
    "    return det(torch.cat((s,x),axis=0))\n",
    " \n",
    "def alcof(M,index):\n",
    "    return pow(-1,index[0]+index[1])*cof1(M,index)\n",
    " \n",
    "def adj(M):\n",
    "    result = torch.zeros((M.shape[0],M.shape[1]))\n",
    "    for i in range(1,M.shape[0]+1):\n",
    "        for j in range(1,M.shape[1]+1):\n",
    "            result[j-1][i-1] = alcof(M,[i,j])\n",
    "    return result\n",
    " \n",
    "def invmat(M):\n",
    "    return 1.0/det(M)*adj(M)\n",
    " \n",
    "M = torch.FloatTensor([[1,2,-1],[2,3,4],[3,1,2]])\n",
    "print(invmat(M))\n",
    "print(torch.inverse(M))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolo5)",
   "language": "python",
   "name": "yolo5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
